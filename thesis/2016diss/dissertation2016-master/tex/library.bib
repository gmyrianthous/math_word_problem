@article{Chiu2015,
abstract = {Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing exact match approaches. Extensive evaluation shows that, given only tokenized text, publicly available word vectors, and an automatically constructed lexicon from open sources, our system is able to surpass the reported state-of-the-art on the OntoNotes 5.0 dataset by 2.35 F1 points and achieves competitive results on the CoNLL 2003 dataset, rivaling systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information.},
archivePrefix = {arXiv},
arxivId = {1511.08308},
author = {Chiu, Jason P. C. and Nichols, Eric},
eprint = {1511.08308},
file = {:home/nathan/Documents/Mendeley Desktop/Q16-1026.pdf:pdf},
issn = {2307-387X},
number = {2003},
pages = {1--9},
title = {{Named Entity Recognition with Bidirectional LSTM-CNNs}},
url = {http://arxiv.org/abs/1511.08308},
volume = {4},
year = {2015}
}
@article{Tsvetkov2013,
author = {Tsvetkov, Yulia and Mukomel, Elena and Gershman, Anatole and Gershman, Ytema},
file = {:home/nathan/Documents/Mendeley Desktop/Tsvetkov et al. - 2013 - Cross-Lingual Metaphor Detection Using Common Semantic Features.pdf:pdf},
journal = {Proceedings of the First Workshop on Metaphor in NLP},
number = {June},
pages = {45--51},
title = {{Cross-Lingual Metaphor Detection Using Common Semantic Features}},
url = {https://www.aclweb.org/anthology/W/W13/W13-09.pdf{\#}page=55$\backslash$nhttp://www.aclweb.org/anthology/W13-0906},
year = {2013}
}
@article{Taghipour2015,
abstract = {One of the weaknesses of current supervised word sense disambiguation (WSD) systems is that they only treat a word as a discrete en-tity. However, a continuous-space represen-tation of words (word embeddings) can pro-vide valuable information and thus improve generalization accuracy. Since word embed-dings are typically obtained from unlabeled data using unsupervised methods, this method can be seen as a semi-supervised word sense disambiguation approach. This paper investi-gates two ways of incorporating word embed-dings in a word sense disambiguation setting and evaluates these two methods on some Sen-sEval/SemEval lexical sample and all-words tasks and also a domain-specific lexical sam-ple task. The obtained results show that such representations consistently improve the ac-curacy of the selected supervised WSD sys-tem. Moreover, our experiments on a domain-specific dataset show that our supervised base-line system beats the best knowledge-based systems by a large margin.},
author = {Taghipour, Kaveh},
file = {:home/nathan/Documents/Mendeley Desktop/Taghipour - 2015 - Semi-Supervised Word Sense Disambiguation Using Word Embeddings in General and Specific Domains.pdf:pdf},
journal = {Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL},
pages = {314--323},
title = {{Semi-Supervised Word Sense Disambiguation Using Word Embeddings in General and Specific Domains}},
year = {2015}
}
@article{Johannsen2014,
author = {Johannsen, Anders and Hovy, Dirk and {Mart$\backslash$'inez Alonso}, H{\'{e}}ctor and Plank, Barbara and S{\o}gaard, Anders},
file = {:home/nathan/Documents/Mendeley Desktop/Johannsen et al. - 2014 - More or less supervised supersense tagging of Twitter.pdf:pdf},
journal = {Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014)},
pages = {1--11},
title = {{More or less supervised supersense tagging of Twitter}},
url = {http://www.aclweb.org/anthology/S14-1001},
year = {2014}
}
@article{Budanitsky2006,
abstract = {The quantification of lexical semantic relatedness has many applications in NLP, and many different measures have been proposed. We evaluate five of these measures, all of which use WordNet as their central resource, by comparing their performance in detecting and correcting real-word spelling errors. An information-content-based measure proposed by Jiang and Conrath is found superior to those proposed by Hirst and St-Onge, Leacock and Chodorow, Lin, and Resnik. In addition, we explain why distributional similarity is not an adequate proxy for lexical semantic relatedness.},
author = {Budanitsky, Alexander and Hirst, Graeme},
doi = {10.1162/coli.2006.32.1.13},
file = {:home/nathan/Documents/Mendeley Desktop/Budanitsky, Hirst - 2006 - Evaluating WordNet-based Measures of Lexical Semantic Relatedness.pdf:pdf},
isbn = {0891-2017},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {1},
pages = {13--47},
pmid = {10602},
title = {{Evaluating WordNet-based Measures of Lexical Semantic Relatedness}},
volume = {32},
year = {2006}
}
@article{Lesk1986,
abstract = {An abstract is not available.},
author = {Lesk, Michael},
doi = {10.1145/318723.318728},
file = {:home/nathan/Documents/Mendeley Desktop/Lesk - 1986 - Automatic Sense Disambiguation Using Machine Readable Dictionaries How to Tell a Pine Cone from an Ice Cream Cone.pdf:pdf},
isbn = {0-89791-224-1},
journal = {Proceedings of the 5th annual international conference on Systems documentation},
keywords = {automatic,dictionaries,disambiguation,machine,readable,sense,using},
pages = {24--26},
title = {{Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone}},
url = {http://dx.doi.org/10.1145/318723.318728},
year = {1986}
}
@article{Ciaramita2006,
abstract = {In this paper we approach word sense disambiguation and information extrac- tion as a unified tagging problem. The task consists of annotating text with the tagset defined by the 41 Wordnet super- sense classes for nouns and verbs. Since the tagset is directly related to Wordnet synsets, the tagger returns partial word sense disambiguation. Furthermore, since the noun tags include the standard named entity detection classes – person, location, organization, time, etc. – the tagger, as a by-product, returns extended named en- tity information. We cast the problem of supersense tagging as a sequential label- ing task and investigate it empirically with a discriminatively-trained Hidden Markov Model. Experimental evaluation on the main sense-annotated datasets available, i.e., Semcor and Senseval, shows consid- erable improvements over the best known “first-sense” baseline.},
author = {Ciaramita, Massimiliano and Altun, Yasemin},
doi = {10.3115/1610075.1610158},
file = {:home/nathan/Documents/Mendeley Desktop/Ciaramita, Altun - 2006 - Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger.pdf:pdf},
isbn = {1-932432-73-6},
journal = {Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing - EMNLP '06},
number = {July},
pages = {594},
title = {{Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger}},
url = {http://portal.acm.org/citation.cfm?doid=1610075.1610158},
year = {2006}
}
@article{Constant2011,
author = {Constant, Matthieu and Sigogne, Anthony},
file = {:home/nathan/Documents/Mendeley Desktop/Constant, Sigogne - 2011 - MWU-aware part-of-speech tagging with a CRF model and lexical resources.pdf:pdf},
journal = {Proceedings of the Workshop on Multiword {\ldots}},
number = {June},
pages = {49--56},
title = {{MWU-aware part-of-speech tagging with a CRF model and lexical resources}},
url = {http://aclweb.org/anthology//W/W11/W11-0809.pdf$\backslash$nhttp://dl.acm.org/citation.cfm?id=2021134},
year = {2011}
}
@article{Schneider2016,
abstract = {This task combines the labeling of multiword expressions and supersenses (coarse-grained classes) in an explicit, yet broad-coverage paradigm for lexical semantics. Nine sys- tems participated; the best scored 57.7{\%} F1 in a multi-domain evaluation setting, indicating that the task remains largely unresolved. An error analysis reveals that a large number of instances in the data set are either hard cases, which no systems get right, or easy cases, which all systems correctly solve. 1},
author = {Schneider, Nathan and Johannsen, Anders and Hovy, Dirk and Carpuat, Marine},
file = {:home/nathan/Documents/Mendeley Desktop/Schneider et al. - 2016 - SemEval-2016 Task 10 Detecting Minimal Semantic Units and their Meanings (DiMSUM).pdf:pdf},
pages = {14},
title = {{SemEval-2016 Task 10: Detecting Minimal Semantic Units and their Meanings (DiMSUM)}},
year = {2016}
}
@article{Ramisch2012,
author = {Ramisch, Carlos and {De Araujo}, Vitor and Villavicencio, Aline},
file = {:home/nathan/Documents/Mendeley Desktop/Ramisch, De Araujo, Villavicencio - 2012 - A broad evaluation of techniques for automatic acquisition of multiword expressions.pdf:pdf},
journal = {Proceedings of the 2012 Student Research Workshop},
number = {July},
pages = {1--6},
title = {{A broad evaluation of techniques for automatic acquisition of multiword expressions}},
url = {http://www.anthology.aclweb.org/W/W12/W12-3301.pdf},
year = {2012}
}
@article{Chih-WeiHsuChih-ChungChang2008,
abstract = {The support vector machine (SVM) is a popular classi cation technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi cant steps. In this guide, we propose a simple procedure which usually gives reasonable results. developed well-differentiated superficial transitional cell bladder cancer. CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to them, because of quality-of-life issues. The incidence of significant complications might not be as high as previously reported, and with a commitment to careful follow-up, SPC can be a safe option for carefully selected patients if adequate surveillance can be ensured.},
author = {{Chih-Wei Hsu, Chih-Chung Chang}, and Chih-Jen Lin},
journal = {BJU international},
number = {1},
pages = {1396--400},
title = {{A Practical Guide to Support Vector Classification}},
url = {http://www.csie.ntu.edu.tw/{~}cjlin/papers/guide/guide.pdf},
volume = {101},
year = {2008}
}
@article{Miller1995,
abstract = {This database links English nouns, verbs, adjectives, and adverbs to sets of synonims that are in turn link through semantic relations that determine word definitions.},
author = {Miller, George a.},
doi = {10.1145/219717.219748},
file = {:home/nathan/Documents/Mendeley Desktop/Miller - 1995 - WordNet a lexical database for English.pdf:pdf},
isbn = {1558602720},
issn = {00010782},
journal = {Communications of the ACM},
number = {11},
pages = {39--41},
pmid = {17081734},
title = {{WordNet: a lexical database for English}},
volume = {38},
year = {1995}
}
@article{Ye2007,
abstract = {We participated in SemEval-1 English coarse-grained all-words task (task 7), English fine-grained all-words task (task 17, subtask 3) and English coarse-grained lexical sample task (task 17, subtask 1). The same method with different labeled data is used for the tasks; SemCor is the labeled corpus used to train our system for the allwords tasks while the labeled corpus that is provided is used for the lexical sample task. The knowledge sources include part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic patterns. In addition, we constructed a topic feature, targeted to capture the global context information, using the latent dirichlet allocation (LDA) algorithm with unlabeled corpus. A modified na ̈ıve Bayes classifier is constructed to incorporate all the features. We achieved 81.6{\%}, 57.6{\%}, 88.7{\%} for coarse-grained allwords task, fine-grained all-words task and coarse-grained lexical sample task respectively. },
author = {Ye, Patrick and Baldwin, Timothy},
file = {:home/nathan/Documents/Mendeley Desktop/Ye, Baldwin - 2007 - MELB-YB Preposition Sense Disambiguation Using Rich Semantic Features.pdf:pdf},
journal = {Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)},
keywords = {preposition; compLx},
number = {June},
pages = {241--244},
title = {{MELB-YB: Preposition Sense Disambiguation Using Rich Semantic Features}},
year = {2007}
}
@article{McCarthy2004,
abstract = {In word sense disambiguation (WSD), the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed. The first (or predominant) sense heuristic assumes the availability of handtagged data. Whilst there are hand-tagged corpora available for some languages, these are relatively small in size and many word forms either do not occur, or occur infrequently. In this paper we investigate the performance of an unsupervised first sense heuristic where predominant senses are acquired automatically from raw text. We evaluate on both the SENSEVAL-2 and SENSEVAL-3 English allwords data. For accurate WSD the first sense heuristic should be used only as a back-off, where the evidence from the context is not strong enough. In this paper however, we examine the performance of the automatically acquired first sense in isolation since it turned out that the first sense taken from SemCor outperformed many systems in SENSEVAL-2.},
author = {McCarthy, Diana and Koeling, Rob and Weeds, Julie and Carroll, John},
file = {:home/nathan/Documents/Mendeley Desktop/McCarthy et al. - 2004 - Using automatically acquired predominant senses for word sense disambiguation.pdf:pdf},
number = {July},
pages = {2--5},
title = {{Using automatically acquired predominant senses for word sense disambiguation}},
url = {http://sro.sussex.ac.uk/21566/},
year = {2004}
}
@article{Kalchbrenner2015,
abstract = {This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images. The network differs from existing deep LSTM architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data. It therefore provides a unified way of using LSTM for both deep and sequential computation. We apply the model to algorithmic tasks such as integer addition and determining the parity of random binary vectors. It is able to solve these problems for 15-digit integers and 250-bit vectors respectively. We then give results for three empirical tasks. We find that 2D Grid LSTM achieves 1.47 bits per character on the Wikipedia character prediction benchmark, which is state-of-the-art among neural approaches. We also observe that a two-dimensional translation model based on Grid LSTM outperforms a phrase-based reference system on a Chinese-to-English translation task, and that 3D Grid LSTM yields a near state-of-the-art error rate of 0.32{\%} on MNIST.},
archivePrefix = {arXiv},
arxivId = {1507.01526},
author = {Kalchbrenner, Nal and Danihelka, Ivo and Graves, Alex},
eprint = {1507.01526},
file = {:home/nathan/Documents/Mendeley Desktop/1507.01526v1.pdf:pdf},
journal = {arXiv preprint arXiv:1507.01526},
pages = {14},
title = {{Grid Long Short-Term Memory}},
url = {http://arxiv.org/abs/1507.01526},
year = {2015}
}
@article{Banarescu2005,
author = {Banarescu, Laura and Bonial, Claire and Knight, Kevin and Palmer, Martha and Schneider, Nathan},
file = {:home/nathan/Documents/Mendeley Desktop/Banarescu et al. - 2005 - Abstract Meaning Representation for Sembanking.pdf:pdf},
title = {{Abstract Meaning Representation for Sembanking}},
year = {2005}
}
@article{Schneider2015,
author = {Schneider, Nathan and Smith, Noah A},
file = {:home/nathan/Documents/Mendeley Desktop/Schneider, Smith - 2015 - A Corpus and Model Integrating Multiword Expressions and Supersenses.pdf:pdf},
journal = {Naacl2015},
pages = {1537--1547},
title = {{A Corpus and Model Integrating Multiword Expressions and Supersenses}},
url = {http://www.cs.cmu.edu/{~}nschneid/sst.pdf},
year = {2015}
}
@article{Cook2007,
author = {Cook, Paul and Fazly, Afsaneh and Stevenson, Suzanne},
file = {:home/nathan/Documents/Mendeley Desktop/Cook, Fazly, Stevenson - 2007 - Pulling their Weight Exploiting Syntactic Forms for the Automatic Identification of Idiomatic Expression.pdf:pdf},
journal = {Proceedings of the Workshop on A Broader Perspective on Multiword Expressions},
number = {June},
pages = {41--48},
title = {{Pulling their Weight: Exploiting Syntactic Forms for the Automatic Identification of Idiomatic Expressions in Context}},
url = {http://www.aclweb.org/anthology/W/W07/W07-1106},
year = {2007}
}
@article{Chung2015,
abstract = {In this paper, we explore the inclusion of random variables into the dynamic latent state of a recurrent neural network (RNN) by combining elements of the varia- tional autoencoder. We argue that through the use of high-level latent random variables, our variational RNN (VRNN) is able to learn to model the kind of vari- ability observed in highly-structured sequential data (such as speech). We em- pirically evaluate the proposed model against related sequential models on five sequence datasets, four of speech and one of handwriting. Our results show the importance of the role random variables can play in the RNN dynamic latent state.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02216v1},
author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
eprint = {arXiv:1506.02216v1},
file = {:home/nathan/Documents/Mendeley Desktop/1506.02216v3.pdf:pdf},
issn = {10495258},
journal = {arXiv},
pages = {1--9},
title = {{A Recurrent Latent Variable Model for Sequential Data}},
year = {2015}
}
@article{Ciaramita2003,
abstract = {We present a new framework for classifying common nouns that extends named-entity classification. We used a fixed set of 26 semantic labels, which we called supersenses. These are the labels used by lexicographers developing WordNet. This framework has a number of practical advantages. We show how information contained in the dictionary can be used as additional training data that improves accuracy in learning new nouns. We also define a more realistic evaluation procedure than cross-validation.},
author = {Ciaramita, Massimiliano and Johnson, Mark},
doi = {10.3115/1119355.1119377},
file = {:home/nathan/Documents/Mendeley Desktop/Ciaramita, Johnson - 2003 - Supersense tagging of unknown nouns in WordNet.pdf:pdf},
journal = {Proceedings of the 2003 conference on Empirical methods in natural language processing -},
pages = {168--175},
title = {{Supersense tagging of unknown nouns in WordNet}},
url = {http://portal.acm.org/citation.cfm?doid=1119355.1119377},
volume = {10},
year = {2003}
}
@article{Berners-Lee2001,
abstract = {A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities.},
author = {Berners-Lee, Tim and Hendler, James and Lassila, Ora},
journal = {Scientific American},
keywords = {agent through the semantic,and,from mom,in a few minutes,indicate terms whose semantics,it,or meaning,pete didn,s place,t like,the agent presented them,the way across town,university hospital was all,web,were defined for the,with a plan},
number = {5},
pages = {34--43},
title = {{The Semantic Web}},
volume = {284},
year = {2001}
}
@article{Turian2010,
abstract = {If we take an existing supervised {\{}NLP{\}} system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and {\{}HLBL{\}} (Mnih {\&} Hinton, 2009) embeddings of words on both {\{}NER{\}} and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing {\{}NLP{\}} systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/},
author = {Turian, Joseph and Ratinov, Lev and Bengio, Yoshua and Turian, Joseph},
doi = {10.1.1.301.5840},
file = {:home/nathan/Documents/Mendeley Desktop/Turian et al. - 2010 - Word Representations A Simple and General Method for Semi-supervised Learning.pdf:pdf},
isbn = {9781617388088},
journal = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
number = {July},
pages = {384--394},
title = {{Word Representations: A Simple and General Method for Semi-supervised Learning}},
url = {http://dl.acm.org/citation.cfm?id=1858721$\backslash$nhttp://dl.acm.org/citation.cfm?id=1858681.1858721},
year = {2010}
}
@article{AdwaitRatnaparkhi1996,
abstract = {This paper presents a statistical model which trains from a corpus annotated with Part-Of-Speech tags and assigns them to previously unseen text with state-of-the-art accuracy (96.6{\%}). The model can be classified as a Maximum Entropy model and simultaneously uses many contextual "features" to predict the POS tag. Furthermore, this paper demonstrates the use of specialized features to model difficult tagging decisions, discusses the corpus consistency problems discovered during the implementation of these features, and proposes a training strategy that mitigates these problems.},
author = {{Adwait Ratnaparkhi}},
file = {:home/nathan/Documents/Mendeley Desktop/Adwait Ratnaparkhi - 1996 - A maximum entropy model for part-of-speech tagging.pdf:pdf},
isbn = {1101145128},
issn = {03645134},
journal = {In Proceedings of the Empirical Methods in Natural Language Processing Conference},
number = {49},
pages = {133--142},
pmid = {581830},
title = {{A maximum entropy model for part-of-speech tagging}},
url = {http://acl.ldc.upenn.edu/W/W96/W96-0213.pdf},
volume = {1},
year = {1996}
}
@article{Baldwin2010,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Baldwin, Timothy and Kim, Su Nam},
doi = {10.1016/B978-0-7020-2797-0.00001-1},
eprint = {arXiv:1011.1669v3},
file = {:home/nathan/Documents/Mendeley Desktop/Baldwin, Kim - 2010 - Multiword Expressions.pdf:pdf},
isbn = {1111001111},
issn = {15710661},
journal = {Handbook of Natural Language Processing, Second Edition.},
pages = {267--292},
pmid = {20314319},
title = {{Multiword Expressions}},
year = {2010}
}
@article{Gregor2014,
abstract = {This paper introduces the Deep Recurrent Atten-tive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distin-guished from real data with the naked eye.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.04623v1},
author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Wierstra, Daan},
eprint = {arXiv:1502.04623v1},
file = {:home/nathan/Documents/Mendeley Desktop/1502.04623.pdf:pdf},
pages = {1--16},
title = {{DRAW: A Recurrent Neural Network For Image Generation}},
year = {2014}
}
@article{Collins2002,
abstract = {We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random fields (CRFs). The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates. We describe theory justifying the algorithms through a modification of the proof of convergence of the perceptron algorithm for classification problems. We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.},
author = {Collins, Michael},
doi = {10.3115/1118693.1118694},
file = {:home/nathan/Documents/Mendeley Desktop/Collins - 2002 - Discriminative training methods for hidden Markov models theory and experiments with perceptron algorithms.pdf:pdf},
journal = {Proceedings of the ACL-02 conference on Empirical methods in natural language processing},
number = {July},
pages = {1--8},
title = {{Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms}},
url = {http://portal.acm.org/citation.cfm?doid=1118693.1118694},
year = {2002}
}
@article{Curran2005,
author = {Curran, James and Curran, James},
file = {:home/nathan/Documents/Mendeley Desktop/Curran, Curran - 2005 - Supersense Tagging of Unknown Words using Semantic Similarity.pdf:pdf},
journal = {Acl'05},
number = {June 2005},
pages = {26--33},
title = {{Supersense Tagging of Unknown Words using Semantic Similarity}},
year = {2005}
}
@article{Levy2015,
abstract = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distri-butional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter op-timizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
file = {:home/nathan/Documents/Mendeley Desktop/Levy, Goldberg, Dagan - 2015 - Improving Distributional Similarity with Lessons Learned from Word Embeddings.pdf:pdf},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
pages = {211--225},
title = {{Improving Distributional Similarity with Lessons Learned from Word Embeddings}},
url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570},
volume = {3},
year = {2015}
}
@article{Jackson2016,
author = {Jackson, Robert B and Carpenter, Stephen R and Dahm, Clifford N and Mcknight, Diane M and Naiman, Robert J and Postel, Sandra L and Running, Steven W and Naiman, Robert J and Postel, Sandra L and Running, Steven W},
file = {:home/nathan/Documents/Mendeley Desktop/416483.pdf:pdf},
number = {4},
pages = {1027--1045},
title = {{Water in a Changing World Published by : Ecological Society of America Stable URL : http://www.jstor.org/stable/3061010 REFERENCES Linked references are available on JSTOR for this article : You may need to log in to JSTOR to access the linked references }},
volume = {11},
year = {2016}
}
@article{Stetina1998,
abstract = {This paper presents a new general supervised word sense disambiguation method based on a relatively small syntactically parsed and semantically tagged training corpus.The method exploits a full sentential context and all the explicit semantic relations in a sentence to identify the senses of all of that sentence's content words. It solves the sparse data problem of a small training corpus by substituting the words by their semantic classes.In spite of a very small training corpus,we report an over- all accuracy of 80.3{\%}(85.7,63.9,83.6 and 86.5{\%},for nouns,verbs,adjectives and adverbs,respectively),which exceeds the accuracy of a statistical sense-frequency based semantic tagging,the only really applicable general disambiguating technique. Because the method uses the sentential syntactic structure it is particularly suitable for integration with a probabilistic syntactic analyser.},
author = {Jiri, Stetina and Makoto, Nagao},
file = {:home/nathan/Documents/Mendeley Desktop/Jiri, Makoto - 1998 - General Word Sense Disambiguation Based on a Full Sentential Context Method Jiri.pdf.pdf:pdf},
keywords = {conceptual hierarchy,d sense disambiguation,discourse,discourse context,relational probability,semantic distance,semantic relations,sense disambiguation,sentential context,word},
mendeley-tags = {conceptual hierarchy,d sense disambiguation,discourse context,relational probability,semantic distance,semantic relations,sentential context},
pages = {47--74},
title = {{General Word Sense Disambiguation Based on a Full Sentential Context Method Jiri.pdf}},
year = {1998}
}
@article{Hovy2010,
author = {Hovy, Dirk and Tratz, Stephen and Hovy, Eduard},
file = {:home/nathan/Documents/Mendeley Desktop/Hovy, Tratz, Hovy - 2010 - What ' s in a Preposition Dimensions of Sense Disambiguation for an Interesting Word Class.pdf:pdf},
journal = {Coling},
number = {August},
pages = {454--462},
title = {{What ' s in a Preposition ? Dimensions of Sense Disambiguation for an Interesting Word Class}},
year = {2010}
}
@article{Heilman2011,
abstract = {Texts with potential educational value are becoming available through the Internet (e.g.,Wikipedia, news services). However, using these new texts in classrooms introduces many challenges, one of which is that they usually lack practice exercises and assessments. Here, we address part of this chal- lenge by automating the creation of a specific type of assessment item. Specifically, we focus on automatically generating factual WH questions. Our goal is to create an automated system that can take as input a text and produce as output questions for assessing a reader's knowledge of the information in the text. The questions could then be presented to a teacher, who could select and revise the ones that he or she judges to be useful. After introducing the problem, we describe some of the computational and linguistic challenges presented by factual question generation. We then present an implemented system that leverages ex- isting natural language processing techniques to address some of these challenges. The system uses a combination of manually encoded transformation rules and a statistical question ranker trained on a tailored dataset of labeled system output. We present experiments that evaluate individual components of the system as well as the system as a whole. We found, among other things, that the question ranker roughly doubled the acceptability rate of top-ranked questions. In a user study, we tested whether K-12 teachers could efficiently create factual questions by se- lecting and revising suggestions from the system. Offering automatic suggestions reduced the time and effort spent by participants, though it also affected the types of questions that were created. This research supports the idea that natural language processing can help teachers efficiently cre- ate instructional content. It provides solutions to some of the major challenges in question generation and an analysis and better understanding of those that remain.},
author = {Heilman, Michael},
file = {:home/nathan/Documents/Mendeley Desktop/Heilman - 2011 - Automatic Factual Question Generation from Text.pdf:pdf},
number = {June},
pages = {203},
title = {{Automatic Factual Question Generation from Text}},
year = {2011}
}
@article{Raskin1995,
abstract = {This work belongs to a family of research efforts, called microtheories and aimed at describing the static meaning of all lexical categories in several languages in the framework of the MikroKosmos project on computational semantics. The latter also in- volves other static microtheories describing world knowledge and syntax-semantics map- ping as well as dynamic microtheories connected with the actual process of text analysis. This paper describes our approach to determining and representing adjectival meaning, compares it with the body of knowledge on adjectives in literature and presents a detailed, practically tested methodology and heuristics for the acquisition of lexical entries for ad- jectives. The work was based on the set of over 6,000 English and about 1,500 Spanish ad- jectives},
author = {Raskin, Victor and Nirenburg, Sergei},
file = {:home/nathan/Documents/Mendeley Desktop/Raskin, Nirenburg - 1995 - Lexical semantics of adjectives.pdf:pdf},
journal = {{\ldots} Research Laboratory Technical Report, MCCS-95 {\ldots}},
pages = {1--69},
title = {{Lexical semantics of adjectives}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.46.6972{\&}rep=rep1{\&}type=pdf},
year = {1995}
}
@article{Katz2006,
abstract = {Making use of latent semantic analysis, we explore the hypothesis that local linguistic context can serve to identify multi-word expressions that have noncompositional meanings. We propose that vector-similarity between distribution vectors associated with an MWE as a whole and those associated with its constitutent parts can serve as a good measure of the degree to which the MWE is compositional. We present experiments that show that low (cosine) similarity does, in fact, correlate with non-compositionality.},
author = {Katz, Graham and Giesbrecht, Eugenie},
doi = {10.3115/1613692.1613696},
file = {:home/nathan/Documents/Mendeley Desktop/Katz, Giesbrecht - 2006 - Automatic identification of non-compositional multi-word expressions using latent semantic analysis.pdf:pdf},
isbn = {1932432841},
journal = {Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties},
number = {July},
pages = {12--19},
title = {{Automatic identification of non-compositional multi-word expressions using latent semantic analysis}},
url = {http://dl.acm.org/citation.cfm?id=1613696},
year = {2006}
}
@misc{ACLWIKI2014,
abstract = {Word Sense Disambiguation (WSD) is the process of identifying the sense of a polysemic word. In modern WSD systems, the senses of a word are typically taken from some specified dictionary. These days WordNet is the usual dictionary in question. WSD has been investigated in computational linguistics as a specific task for well over 40 years, though the acronym is newer. The SENSEVAL conferences have attempted to put Word Sense Disambiguation on an empirically measurable basis by hosting evaluations in which a given corpus of tagged word senses are created using WordNet's senses and participants attempt to recognize those senses after tuning their systems with a corpus of training data.},
author = {{ACL WIKI}},
booktitle = {ACL Wiki},
title = {{Word Sense Disambiguation}},
url = {http://aclweb.org/aclwiki/index.php?title=Word{\_}sense{\_}disambiguation},
year = {2014}
}
@article{Curran2005a,
abstract = {The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words. Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET. Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples. We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but significantly outperforms their tagger. We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semantic similarity.},
author = {Curran, James R.},
doi = {10.3115/1219840.1219844},
file = {:home/nathan/Documents/Mendeley Desktop/Curran - 2005 - Supersense Tagging of Unknown Nouns using Semantic Similarity.pdf:pdf},
isbn = {1932432515},
journal = {Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL)},
number = {2001},
pages = {26--33},
title = {{Supersense Tagging of Unknown Nouns using Semantic Similarity}},
year = {2005}
}
@article{Marcus1993,
abstract = {There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenom- ena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large cor- pora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valu- able for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investi- gation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.},
author = {Marcus, Mitchell and Santorini, Beatrice and Marcinkiewicz, Mary},
doi = {10.1162/coli.2010.36.1.36100},
file = {:home/nathan/Documents/Mendeley Desktop/Marcus, Santorini, Marcinkiewicz - 1993 - Building a large annotated corpus of English the Penn Treebank.pdf:pdf},
isbn = {0891-2017},
issn = {08912017},
journal = {Computational Linguistics},
keywords = {POS-Tagging},
number = {2},
pages = {313--330},
title = {{Building a large annotated corpus of English : the Penn Treebank.}},
url = {http://dblp.uni-trier.de/db/journals/coling/coling19.html{\#}MarcusSM94},
volume = {19},
year = {1993}
}
@article{Tsatsaronis2007,
abstract = {Most word sense disambiguation (WSD) methods require large quantities of manually annotated training data and/or do not exploit fully the semantic relations of thesauri. We propose a new unsupervised WSD algorithm, which is based on generating Spreading Activation Networks (SANs) from the senses of a thesaurus and the relations between them. A new method of assigning weights to the networks{\^{a}}€™ links is also proposed. Experiments show that the algorithm outperforms previous unsupervised approaches to WSD.},
author = {Tsatsaronis, George and Vazirgiannis, Michalis and Androutsopoulos, Ion},
doi = {10.1145/1459352.1459355},
file = {:home/nathan/Documents/Mendeley Desktop/Tsatsaronis, Vazirgiannis, Androutsopoulos - 2007 - Word sense disambiguation with spreading activation networks generated from thesauri.pdf:pdf},
isbn = {0360-0300},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
number = {2},
pages = {1725--1730},
title = {{Word sense disambiguation with spreading activation networks generated from thesauri}},
volume = {41},
year = {2007}
}
@article{Cruse1986,
author = {Cruse, Alan},
file = {:home/nathan/Documents/Mendeley Desktop/Cruse - 1986 - Lexical Semantics.pdf:pdf},
isbn = {052125678X},
title = {{Lexical Semantics}},
year = {1986}
}
@article{Tsvetkov2014,
author = {Tsvetkov, Yulia and Schneider, Nathan and Hovy, Dirk},
file = {:home/nathan/Documents/Mendeley Desktop/Tsvetkov, Schneider, Hovy - 2014 - Augmenting English adjective senses with supersenses.pdf:pdf},
journal = {Lrec},
keywords = {adjective supersenses,lexical semantics,semantic taxonomy induction},
title = {{Augmenting English adjective senses with supersenses}},
url = {http://demo.clab.cs.cmu.edu/cdyer/adj-lrec14.pdf},
year = {2014}
}
@article{Jurafsky2015,
abstract = {Fuzzy logic refers to a computer's ability to make decisions involving$\backslash$n"grey" or "fuzzy" areas. As linguistics contains numerous "grey"$\backslash$nareas, computing with words through the use of fuzzy logic is an$\backslash$nextremely hot topic in database and Internet research. This book$\backslash$nexplores the state of the art in linguistic computation, discussing$\backslash$nhow current research findings are extending the application of fuzzy$\backslash$nlogic beyond control engineering and intelligent systems into the$\backslash$nuse of language on a computer. Fuzzy logic pioneer, Dr. Lofti Zadeh,$\backslash$nprovides the introduction for this thought-provoking work.},
author = {Jurafsky, Daniel and Martin, James H.},
file = {:home/nathan/Documents/Mendeley Desktop/Jurafsky, Martin - 2015 - Computing with Word Senses.pdf:pdf},
journal = {Speech and Language Processing},
title = {{Computing with Word Senses}},
year = {2015}
}
@article{Nastase2008,
abstract = {We present experiments that analyze the necessity of using a highly interconnected word/sense graph for unsupervised all- words word sense disambiguation. We show that allowing only grammatically related words to in uence each other{\^{a}}€™s senses leads to disambiguation results on a par with the best graph-based systems, while greatly re- ducing the computation load. We also com- pare two methods for computing selectional preferences between the senses of every two grammatically related words: one using a Lesk-based measure on WordNet, the other using dependency relations from the British National Corpus. The best con guration uses the syntactically-constrained graph, se- lectional preferences computed from the corpus and a PageRank tie-breaking algo- rithm. We especially note good performance when disambiguating verbs with grammati- cally constrained links. },
author = {Nastase, Vivi},
file = {:home/nathan/Documents/Mendeley Desktop/Nastase - 2008 - Unsupervised All-words Word Sense Disambiguation with Grammatical Dependencies.pdf:pdf},
journal = {Ijcnlp},
pages = {757--762},
title = {{Unsupervised All-words Word Sense Disambiguation with Grammatical Dependencies}},
year = {2008}
}
@article{Moschitti2004,
abstract = {In this paper we have designed and experimented novel convolution$\backslash$nkernels for automatic classification of predicate arguments. Their$\backslash$nmain property is the ability to process structured representations.$\backslash$nSupport Vector Machines (SVMs), using a combination of such kernels$\backslash$nand the flat feature kernel, classify Prop-Bank predicate arguments$\backslash$nwith accuracy higher than the current argument classification state-of-the-art.Additionally,$\backslash$nexperiments on FrameNet data have shown that SVMs are appealing for$\backslash$nthe classification of semantic roles even if the proposed kernels$\backslash$ndo not produce any improvement.},
author = {Moschitti, Alessandro},
doi = {http://dx.doi.org/10.3115/1218955.1218998},
file = {:home/nathan/Documents/Mendeley Desktop/Moschitti - 2004 - A study on convolution kernels for shallow semantic parsing.pdf:pdf},
isbn = {3-540-45375-X},
journal = {ACL '04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics},
keywords = {tree{\_}kernel},
pages = {335},
title = {{A study on convolution kernels for shallow semantic parsing}},
url = {http://dx.doi.org/10.3115/1218955.1218998$\backslash$nhttp://aclweb.org/anthology/P04-1043$\backslash$nhttp://portal.acm.org/citation.cfm?id=1218998{\#}},
year = {2004}
}
@article{Schneider2014a,
abstract = {Multiword expressions (MWEs) are quite frequent in languages such as English, but their diversity, the scarcity of individual MWE types, and contextual ambiguity have presented obstacles to corpus-based studies and NLP systems addressing them as a class. Here we advocate for a comprehensive annotation approach: proceeding sentence by sentence, our annotators manually group tokens into MWEs according to guidelines that cover a broad range of multiword phenomena. Under this scheme, we have fully annotated an English web corpus for multiword expressions, including those containing gaps.},
author = {Schneider, Nathan and Onuffer, Spencer and Kazour, Nora and Danchik, Emily and Mordowanec, M. T. and Conrad, Henrietta and Smith, N. A.},
file = {:home/nathan/Documents/Mendeley Desktop/Schneider et al. - 2014 - Comprehensive Annotation of Multiword Expressions in a Social Web Corpus.pdf:pdf},
isbn = {978-2-9517408-8-4},
journal = {Proceedings of the Ninth International Conference on Language Resources and Evaluation},
keywords = {corpus annotation,multiword expressions,social media},
pages = {455--461},
title = {{Comprehensive Annotation of Multiword Expressions in a Social Web Corpus}},
url = {http://server01.ark.cs.cmu.edu/LexSem/mwecorpus.pdf},
year = {2014}
}
@article{Mcdonald2005,
abstract = {Untyped dependency parsing can be viewed as the problem of finding maximum spanning trees (MSTs) in directed graphs. Using this representation, the Eisner (1996) parsing algorithm is sufficient for searching the space of projective trees. More importantly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm. These efficient parse search methods support large-margin discriminative training methods for learning dependency parsers. We evaluate these methods experimentally on the English and Czech treebanks.},
author = {Mcdonald, Ryan},
file = {:home/nathan/Documents/Mendeley Desktop/Mcdonald - 2005 - Spanning Tree Methods for Discriminative Training of Dependency Parsers.pdf:pdf},
journal = {Technical Report},
title = {{Spanning Tree Methods for Discriminative Training of Dependency Parsers}},
year = {2005}
}
@article{Peters2000,
author = {Peters, I and Peters, Wim},
file = {:home/nathan/Documents/Mendeley Desktop/Peters, Peters - 2000 - The treatment of adjectives in SIMPLE theoretical observations.pdf:pdf},
journal = {Proceedings of LREC},
title = {{The treatment of adjectives in SIMPLE: theoretical observations}},
url = {http://gandalf.aksis.uib.no/non/lrec2000/pdf/366.pdf},
year = {2000}
}
@article{Sak2014,
abstract = {Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.1128v1},
author = {Sak, Ha$\backslash$csim and Senior, Andrew and Beaufays, Fran{\c{c}}oise},
doi = {arXiv:1402.1128},
eprint = {arXiv:1402.1128v1},
file = {:home/nathan/Documents/Mendeley Desktop/1402.1128v1.pdf:pdf},
journal = {arXiv preprint arXiv:1402.1128},
number = {Cd},
title = {{Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition}},
year = {2014}
}
@article{Pradhan2004,
author = {Pradhan, Sameer and Ward, Wayne and Hacioglu, Kadri and Martin, James H. and Jurafsky, Dan},
file = {:home/nathan/Documents/Mendeley Desktop/Pradhan et al. - 2004 - Shallow Semantic Parsing Using Support Vector Machines.pdf:pdf},
journal = {Proceedings of the Human Language Technology Conference/North American chapter of the Association of Computational Linguistics},
title = {{Shallow Semantic Parsing Using Support Vector Machines}},
url = {http://scholar.google.de/scholar?hl=de{\&}q=jurafsky+gildea+2000{\&}btnG=Suche{\&}lr={\&}as{\_}ylo={\&}as{\_}vis=0{\#}0},
year = {2004}
}
@article{Xu1994,
abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the cor- responding words in the output sequence. We validate the use of attention with state-of-the- art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO. 1.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Xu, Kelvin and Kiros, Jimmy Lei Ba Ryan and Courville, Kyunghyun Cho Aaron and Bengio, Ruslan Salakhutdinov Richard S. Zemel Yoshua},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:home/nathan/Documents/Mendeley Desktop/1502.03044v2.pdf:pdf},
isbn = {1045-9227 VO - 5},
issn = {19410093},
journal = {IEEE Transactions on Neural Networks},
number = {2},
pages = {157--166},
pmid = {18267787},
title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
volume = {5},
year = {1994}
}
@article{Marcus,
author = {Marcus, Mitchell P},
file = {:home/nathan/Documents/Mendeley Desktop/Marcus - Unknown - Text Chunking using Transformation-Based Learning 1 Introduction.pdf:pdf},
pages = {82--94},
title = {{Text Chunking using Transformation-Based Learning 1 Introduction}},
year = {1999}
}
@article{Collins2013,
author = {Collins, Michael},
file = {:home/nathan/Documents/Mendeley Desktop/Collins - 2013 - Log-Linear Models, MEMMs, and CRFs.pdf:pdf},
pages = {1--11},
title = {{Log-Linear Models, MEMMs, and CRFs}},
year = {2013}
}
@article{Shi2005,
abstract = {This paper describes our work in integrating three different lexical resources: FrameNet, VerbNet, and WordNet, into a unified, richer knowledge-base, to the end of enabling more robust semantic parsing. The construction of each of these lexical resources has required many years of laborious human effort, and they all have their strengths and shortcomings. By linking them together, we build an improved resource in which (1) the coverage of FrameNet is extended, (2) the VerbNet lexicon is augmented with frame semantics, and (3) selectional restrictions are implemented using WordNet semantic classes. The synergistic exploitation of various lexical resources is crucial for many complex language processing applications, and we prove it once again effective in building a robust semantic parser.},
author = {Shi, Lei and Mihalcea, Rada},
doi = {10.1007/978-3-540-30586-6_9},
file = {:home/nathan/Documents/Mendeley Desktop/Shi, Mihalcea - 2005 - Putting Pieces Together Combining FrameNet , VerbNet and WordNet.pdf:pdf},
isbn = {3-540-24523-5, 978-3-540-24523-0},
issn = {03029743},
journal = {Computational Linguistics and Intelligent Text Processing},
pages = {100--111},
title = {{Putting Pieces Together : Combining FrameNet , VerbNet and WordNet}},
year = {2005}
}
@article{Agirre2010,
abstract = {Domain portability and adaptation of NLP components and Word Sense Disambiguation systems present new challenges. The difficulties found by supervised systems to adapt might change the way we assess the strengths and weaknesses of supervised and knowledgebased WSD systems. Unfortunately, all existing evaluation datasets for specific domains are lexical-sample corpora. With this paper we want to motivate the creation of an allwords test dataset for WSD on the environment domain in several languages, and present the overall design of this SemEval task.},
author = {Agirre, Eneko and de Lacalle, Oier Lopez and Fellbaum, Christiane and Marchetti, Andrea and Toral, Antonio and Vossen, Piek},
file = {:home/nathan/Documents/Mendeley Desktop/Agirre et al. - 2010 - SemEval-2010 Task 17 All-words Word Sense Disambiguation on a Specific Domain.pdf:pdf},
isbn = {978-1-932432-31-2},
journal = {Proceedings of the 5th International Workshop on Semantic Evaluation},
number = {July},
pages = {75--80},
title = {{SemEval-2010 Task 17: All-words Word Sense Disambiguation on a Specific Domain}},
url = {papers2://publication/uuid/B39F0993-46B5-4727-9761-E2D9629D1DB1},
year = {2010}
}
@article{Goldberg2015,
author = {Goldberg, Yoav},
file = {:home/nathan/Documents/Mendeley Desktop/Goldberg - 2015 - Goldberg{\_}Nnlp.pdf:pdf},
pages = {1--76},
title = {{Goldberg{\_}Nnlp}},
year = {2015}
}
@article{Burges1998,
abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Burges, C.J.C. J Christopher J C},
doi = {10.1023/A:1009715923555},
eprint = {1111.6189v1},
isbn = {0818672404},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {pattern recognition,statistical learning theory,support vector machines,vc dimension},
number = {2},
pages = {121--167},
pmid = {5207842081938259593},
title = {{A tutorial on support vector machines for pattern recognition}},
url = {http://www.springerlink.com/index/Q87856173126771Q.pdf},
volume = {2},
year = {1998}
}
@article{Graves2005,
abstract = {In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it. ?? 2005 Elsevier Ltd. All rights reserved.},
author = {Graves, Alex and Schmidhuber, J??rgen},
doi = {10.1109/IJCNN.2005.1556215},
file = {:home/nathan/Documents/Mendeley Desktop/Graves2005a.pdf:pdf},
isbn = {0780390482},
issn = {08936080},
journal = {Proceedings of the International Joint Conference on Neural Networks},
pages = {2047--2052},
pmid = {16112549},
title = {{Framewise phoneme classification with bidirectional LSTM networks}},
volume = {4},
year = {2005}
}
@article{Green2013,
author = {Green, Spence and de Marneffe, Marie-Catherine and Manning, Christopher D.},
doi = {10.1162/COLI_a_00139},
file = {:home/nathan/Documents/Mendeley Desktop/Green, de Marneffe, Manning - 2013 - Parsing Models for Identifying Multiword Expressions.pdf:pdf},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {1},
pages = {195--227},
title = {{Parsing Models for Identifying Multiword Expressions}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/COLI{\_}a{\_}00139},
volume = {39},
year = {2013}
}
@article{Schneider2012,
author = {Schneider, Nathan and Mohit, Behrang and Oflazer, Kemal and Smith, Noah A},
file = {:home/nathan/Documents/Mendeley Desktop/Schneider et al. - 2012 - Coarse lexical semantic annotation with supersenses an {\{}A{\}}rabic case study.pdf:pdf},
isbn = {9781937284251},
journal = {Proc. of {\{}Proceedings of ACL{\}}},
pages = {253--258},
title = {{Coarse lexical semantic annotation with supersenses: an {\{}A{\}}rabic case study}},
year = {2012}
}
@article{Bayer2014,
abstract = {Leveraging advances in variational inference, we propose to enhance recurrent neural networks with latent variables, resulting in Stochastic Recurrent Networks (STORNs). The model i) can be trained with stochastic gradient methods, ii) allows structured and multi-modal conditionals at each time step, iii) features a reliable estimator of the marginal likelihood and iv) is a generalisation of deterministic recurrent neural networks. We evaluate the method on four polyphonic musical data sets and motion capture data.},
archivePrefix = {arXiv},
arxivId = {1411.7610},
author = {Bayer, Justin and Osendorfer, Christian},
eprint = {1411.7610},
file = {:home/nathan/Documents/Mendeley Desktop/1411.7610v3.pdf:pdf},
pages = {1--9},
title = {{Learning Stochastic Recurrent Networks}},
url = {http://arxiv.org/abs/1411.7610},
year = {2014}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Fi-nally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
doi = {10.1007/s10107-014-0839-0},
eprint = {1409.3215},
file = {:home/nathan/Documents/Mendeley Desktop/5346-sequence-to-sequence-learning-with-neural-networks.pdf:pdf},
isbn = {1409.3215},
issn = {09205691},
journal = {Advances in Neural Information Processing Systems (NIPS)},
pages = {3104--3112},
pmid = {2079951},
title = {{Sequence to sequence learning with neural networks}},
url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural},
year = {2014}
}
@article{Ramshaw1994,
abstract = {Eric Brill introduced transformation-based learning and showed that it can do part-of-speech tagging with fairly high accuracy. The same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive ``baseNP'' chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 92{\%} for baseNP chunks and 88{\%} for somewhat more complex chunks that partition the sentence. Some interesting adaptations to the transformation-based learning approach are also suggested by this application.},
archivePrefix = {arXiv},
arxivId = {cmp-lg/9505040},
author = {Ramshaw, Lance A. and Marcus, Mitchell P.},
eprint = {9505040},
file = {:home/nathan/Documents/Mendeley Desktop/Ramshaw, Marcus - 1994 - Text Chunking using Transformation-Based Learning.pdf:pdf},
journal = {Proceedings of the Third A CL Workshop on Very Large Corpora.},
pages = {13},
primaryClass = {cmp-lg},
title = {{Text Chunking using Transformation-Based Learning}},
year = {1995}
}
@article{Tratz2010,
author = {Tratz, Stephen and Hovy, Eduard},
file = {:home/nathan/Documents/Mendeley Desktop/Tratz, Hovy - 2010 - ISI automatic classification of relations between nominals using a maximum entropy classifier.pdf:pdf},
journal = {Proceedings of the 5th International Workshop on Semantic Evaluation},
number = {July},
pages = {222--225},
title = {{ISI: automatic classification of relations between nominals using a maximum entropy classifier}},
year = {2010}
}
@article{Mihalcea2007,
abstract = {This paper describes a method for generating sense-tagged data using Wikipedia as a source of sense annotations. Through word sense disambiguation experiments, we show that the Wikipedia-based sense annotations are reliable and can be used to construct accurate sense classifiers.},
author = {Mihalcea, Rada},
doi = {10.3115/1117794.1117812},
file = {:home/nathan/Documents/Mendeley Desktop/Mihalcea - 2007 - Using Wikipedia for Automatic Word Sense Disambiguation.pdf:pdf},
journal = {Hlt-Naacl},
pages = {196--2003},
title = {{Using Wikipedia for Automatic Word Sense Disambiguation}},
year = {2007}
}
@article{Owoputi2012,
abstract = {We present improvements to a Twitter part-of-speech tagger, making use of several new features and large- scale word clustering. With these changes, the tagging accuracy increased from 89.2{\%} to 92.8{\%} and the tagging speed is 40 times faster. In addition, we expanded our Twitter tokenizer to support a broader range of Unicode characters, emoticons, and URLs. Finally, we annotate and evaluate on a new tweet dataset, DAILYTWEET547, that is more statistically representative of English-language Twitter as a whole. The new tagger is released as TweetNLP version 0.3, along with the new annotated data and large-scale word clusters at http://www.ark.cs.cmu.edu/TweetNLP. This},
author = {Owoputi, Olutobi and O'Connor, Brendan and Dyer, Chris and Gimpel, Kevin and Schneider, Nathan},
file = {:home/nathan/Documents/Mendeley Desktop/Owoputi et al. - 2012 - Part-of-Speech Tagging for Twitter Word Clusters and Other Advances.pdf:pdf},
journal = {Cmu-Ml-12-107},
title = {{Part-of-Speech Tagging for Twitter: Word Clusters and Other Advances}},
year = {2012}
}
@article{Han2009,
abstract = {Name ambiguity problem has raised an urgent demand for efficient, high-quality named entity disambiguation methods. The key problem of named entity disambiguation is to measure the similarity between occurrences of names. The traditional methods measure the similarity using the bag of words (BOW) model. The BOW, however, ignores all the semantic relations such as social relatedness between named entities, associative relatedness between concepts, polysemy and synonymy between key terms. So the BOW cannot reflect the actual similarity. Some research has investigated social networks as background knowledge for disambiguation. Social networks, however, can only capture the social relatedness between named entities, and often suffer the limited coverage problem. To overcome the previous methods' deficiencies, this paper proposes to use Wikipedia as the background knowledge for disambiguation, which surpasses other knowledge bases by the coverage of concepts, rich semantic information and up-to-date content. By leveraging Wikipedia's semantic knowledge like social relatedness between named entities and associative relatedness between concepts, we can measure the similarity between occurrences of names more accurately. In particular, we construct a large-scale semantic network from Wikipedia, in order that the semantic knowledge can be used efficiently and effectively. Based on the constructed semantic network, a novel similarity measure is proposed to leverage Wikipedia semantic knowledge for disambiguation. The proposed method has been tested on the standard WePS data sets. Empirical results show that the disambiguation performance of our method gets 10.7{\%} improvement over the traditional BOW based methods and 16.7{\%} improvement over the traditional social network based methods.},
author = {Han, Xianpei and Zhao, Jun},
doi = {10.1145/1645953.1645983},
file = {:home/nathan/Documents/Mendeley Desktop/Han, Zhao - 2009 - Named entity disambiguation by leveraging wikipedia semantic knowledge.pdf:pdf},
isbn = {9781605585123},
journal = {Proceeding of the 18th ACM conference on Information and knowledge management - CIKM '09},
keywords = {coreference,name ambiguity,named entity disambiguation,record linkage,resolution,semantic knowledge},
pages = {215--224},
title = {{Named entity disambiguation by leveraging wikipedia semantic knowledge}},
url = {http://portal.acm.org/citation.cfm?id=1645953.1645983$\backslash$nhttp://portal.acm.org/citation.cfm?doid=1645953.1645983},
year = {2009}
}
@article{Schneider2014,
author = {Schneider, Nathan and Danchik, Emily and Dyer, Chris and Smith, Noah a},
file = {:home/nathan/Documents/Mendeley Desktop/Schneider et al. - 2014 - Discriminative Lexical Semantic Segmentation with Gaps Running the MWE Gamut.pdf:pdf},
issn = {2307-387X},
journal = {Acl2014},
pages = {193--206},
title = {{Discriminative Lexical Semantic Segmentation with Gaps: Running the MWE Gamut}},
volume = {2},
year = {2014}
}
@article{Le2014,
author = {Le, Quoc V},
file = {:home/nathan/Documents/Mendeley Desktop/Le - 2014 - A Tutorial on Deep Learning Part 1 Nonlinear Classifiers and The Backpropagation Algorithm.pdf:pdf},
pages = {1--18},
title = {{A Tutorial on Deep Learning Part 1: Nonlinear Classifiers and The Backpropagation Algorithm}},
year = {2014}
}
@article{McDonald2007,
abstract = {We present a comparative error analysis of the two dominant approaches in datadriven dependency parsing: global, exhaustive, graph-based models, and local, greedy, transition-based models. We show that, in spite of similar performance overall, the two models produce different types of errors, in a way that can be explained by theoretical properties of the two models. This analysis leads to new directions for parser development.},
author = {McDonald, Ryan and Nivre, Joakim},
file = {:home/nathan/Documents/Mendeley Desktop/McDonald, Nivre - 2007 - Characterizing the Errors of Data-Driven Dependency Parsing Models.pdf:pdf},
journal = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)},
number = {June},
pages = {122--131},
title = {{Characterizing the Errors of Data-Driven Dependency Parsing Models}},
url = {http://www.aclweb.org/anthology/D/D07/D07-1013},
year = {2007}
}
@article{Schneider2015a,
author = {Schneider, Nathan and Smith, Noah A},
file = {:home/nathan/Documents/Mendeley Desktop/Schneider, Smith - 2015 - A Corpus and Model Integrating Multiword Expressions and Supersenses(2).pdf:pdf},
journal = {Naacl2015},
pages = {1537--1547},
title = {{A Corpus and Model Integrating Multiword Expressions and Supersenses}},
url = {http://www.cs.cmu.edu/{~}nschneid/sst.pdf},
year = {2015}
}
@article{Lample2016,
abstract = {State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.},
archivePrefix = {arXiv},
arxivId = {1603.01360},
author = {Lample, Guillaume},
eprint = {1603.01360},
file = {:home/nathan/Documents/Mendeley Desktop/1603.01360v3.pdf:pdf},
journal = {Arxiv},
pages = {1--10},
title = {{Neural Architectures for Named Entity Recognition}},
year = {2016}
}
@article{Zhong2009,
abstract = {While the most accurate word sense disambiguation systems are built using supervised learning from sense-tagged data, scaling them up to all words of a language has proved elusive, since preparing a sense-tagged corpus for all words of a language is time-consuming and human labor intensive. In this paper, we propose and implement a completely automatic approach to scale up word sense disambiguation to all words of English. Our approach relies on English-Chinese parallel corpora, English-Chinese bilingual dictionaries, and automatic methods of finding synonyms of Chinese words. No additional human sense annotations or word translations are needed. We conducted a large-scale empirical evaluation on more than 29,000 noun tokens in English texts annotated in OntoNotes 2.0, based on its coarse-grained sense inventory. The evaluation results show that our approach is able to achieve high accuracy, outperforming the first-sense baseline and coming close to a prior reported approach that requires manual human efforts to provide Chinese translations of English senses.},
author = {Zhong, Zhi and Ng, Hwee Tou},
file = {:home/nathan/Documents/Mendeley Desktop/Zhong, Ng - 2009 - Word sense disambiguation for all words without hard labor.pdf:pdf},
isbn = {9781577354260},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Natural-Language Processing},
pages = {1616--1621},
title = {{Word sense disambiguation for all words without hard labor}},
year = {2009}
}
@article{Kilgarriff2014,
abstract = {The Sketch Engine is a leading corpus tool, widely used in lexicography. Now, at 10 years old, it is mature software. The Sketch Engine website offers many ready-to-use corpora, and tools for users to build, upload and install their own corpora. The paper describes the core functions (word sketches, concordancing, thesaurus). It outlines the different kinds of users, and the approach taken to working with many different languages. It then reviews the kinds of corpora available in the Sketch Engine, gives a brief tour of some of the innovations from the last few years, and surveys other corpus tools and websites.},
author = {Kilgarriff, Adam and Baisa, V{\'{i}}t and Bu{\v{s}}ta, Jan and Jakub{\'{i}}{\v{c}}ek, Milo{\v{s}} and Kov{\'{a}}ř, Vojt{\v{e}}ch and Michelfeit, Jan and Rychl{\'{y}}, Pavel and Suchomel, V{\'{i}}t},
doi = {10.1007/s40607-014-0009-9},
file = {:home/nathan/Documents/Mendeley Desktop/Kilgarriff et al. - 2014 - The Sketch Engine ten years on.pdf:pdf},
issn = {2197-4292},
journal = {Lexicography},
keywords = {corpora,corpus lexicography,corpus tools,word sketches},
number = {2012},
pages = {7--36},
title = {{The Sketch Engine: ten years on}},
url = {http://link.springer.com/10.1007/s40607-014-0009-9},
volume = {1},
year = {2014}
}
@article{Sag2002,
abstract = {Abstract. Multiword expressions are a key problem for the develop- ment of large-scale, linguistically sound natural language processing tech- nology. This paper surveys the problem and some currently available analytic techniques. The various kinds of multiword expressions should be analyzed in distinct ways, including listing “words with spaces”, hi- erarchically organized lexicons, restricted combinatoric rules, lexical se- lection, “idiomatic constructions” and simple statistical affinity. An ad- equate comprehensive analysis of multiword expressions must employ both symbolic and statistical techniques.},
author = {Sag, I and Baldwin, Timothy and Bond, Francis},
doi = {10.1162/COLI_a_00139},
file = {:home/nathan/Documents/Mendeley Desktop/Sag, Baldwin, Bond - 2002 - Multiword expressions A pain in the neck for NLP.pdf:pdf},
isbn = {3-540-43219-1},
issn = {0891-2017},
journal = {{\ldots} Linguistics and Intelligent {\ldots}},
pages = {1--15},
title = {{Multiword expressions: A pain in the neck for NLP}},
url = {http://www.springerlink.com/index/k7etlqv25lxj3j1w.pdf},
year = {2002}
}
@article{OHara2009,
abstract = {This article describes how semantic role resources can be exploited for preposition disambigua- tion. The main resources include the semantic role annotations provided by the Penn Treebank and FrameNet tagged corpora. The resources also include the assertions contained in the Fac- totum knowledge base, as well as information from Cyc and Conceptual Graphs. A common inventory is derived from these in support of definition analysis, which is the motivation for this work. The disambiguation concentrates on relations indicated by prepositional phrases, and is framed as word-sense disambiguation for the preposition in question. A new type of feature for word-sense disambiguation is introduced, usingWordNet hypernyms as collocations rather than just words. Various experiments over the Penn Treebank and FrameNet data are presented, in- cluding prepositions classified separately versus together, and illustrating the effects of filtering. Similar experimentation is done over the Factotum data, including a method for inferring likely preposition usage from corpora, as knowledge bases do not generally indicate how relationships are expressed in English (in contrast to the explicit annotations on this in the Penn Treebank and FrameNet). Other experiments are included with the FrameNet data mapped into the common relation inventory developed for definition analysis, illustrating how preposition disambiguation might be applied in lexical acquisition. 1.},
author = {O'Hara, Tom and Wiebe, Janyce},
doi = {10.1162/coli.06-79-prep15},
file = {:home/nathan/Documents/Mendeley Desktop/O'Hara, Wiebe - 2009 - Exploiting Semantic Role Resources for Preposition Disambiguation.pdf:pdf},
isbn = {0891-2017},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {2},
pages = {151--184},
title = {{Exploiting Semantic Role Resources for Preposition Disambiguation}},
volume = {35},
year = {2009}
}
@article{DanielJurafsky2015,
author = {{Daniel Jurafsky} and {James H. Martin}},
file = {:home/nathan/Documents/Mendeley Desktop/Daniel Jurafsky, James H. Martin - 2015 - Vector Semantics.pdf:pdf},
journal = {Speech and Language Processing},
title = {{Vector Semantics}},
year = {2015}
}
