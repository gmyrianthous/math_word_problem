\chapter{Conclusions}\label{chapter6}

The conclusions from performing a research based dissertation project are inherently theoretical, but practical ideas about tools, programming, development and organisation were learned along the way. In this chapter, I will try to address the importance of these areas. I will begin with the basic overview of results for the task, then I will speak about lessons learned in software development, move on to speaking about the models and data usage and finally lead into the importance of research and analysis.

\section{Task results}

The highest scoring CRF baseline produced an F1-Score of 47.8\% while the final Bi-LSTM-CRF produced a F1-Score of 53.2\%. A final increase of 5.4\% is within the margins that I wished to obtain, which was within the limits of the original \dimsum publication\footnote{F1-Scores ranged from 25.71\% to 57.77\%}.

Initially, the first baseline system approach seemed the most logical by predicting in two passes, but interestingly enough, the third baseline with joint tag labels gave the highest evaluated result. The most simplistic baseline model turned out to be the best in a quantifiable domain, when the implication for such a choice was not always clear initially. Analysis of results helps determine why experimental choices potentially provide better solutions to tasks. 

A clear implication in the analysis section shows that the final Bi-LSTM-CRF produced higher results due to pre-computed word embeddings. Other optimisations gave small improvements of a few percent, but were not nearly comparable to the almost 9\% increase in F1-Score. 

Although I am content to have received a positive improvement, the experience of trying to solve the \dimsum task was equally important.

\section{Experience}

Conclusions based on the actions involved in a research project can be interpreted from different viewpoints. This section intends to describe conclusions from alternative interpretive approaches.

\subsection{Software Development}

On multiple occasions, in multiple courses and personal projects, I try to use modular code to try and remove dependencies where they shouldn't be and create modules that can be used for many tasks. 

Initial modules used for the baseline proved very important when creating alternative baseline systems to experiment with feature engineering and evaluation. After adapting the previous NER system, the previous modules developed in \texttt{Python} were not always usable due to difference in data handling. This is most likely due to not writing generic enough modules for usage with other systems and not having a standardised API. 

After capturing results for many models, additional scripts were created for gathering statistics on the corpus for analysis. These scripts were thought to be of use in only one or two very specific cases whereas they turned out to be more useful then originally thought and proper refactoring and modularisation of code should have been done initially. 

The development of \texttt{Python} code and adaption of preexisting code helped me understand novel code creation and gave a broader comprehension of previously developed code. 

\subsection{Data and Models}

Understanding data for a research task is key, and picking the correct model is the door. Not all data fits well with every model, and each model cannot accept every type of data structure. Having a deep understanding of data allows for correct model selection. I believe spending more time on understanding data and its structure is paramount to successful analysis and evaluation. In future tasks, this will not be overlooked and more time will be spent to create a deeper understanding before model selection. 

Due to the static methods of features in model choices made for this task, potential decreases in results could occur. As mentioned in the analysis, using dynamic models could help create better results. In future research and projects, model flexibility will not be overlooked as robustness itself is a feature work exploiting.

\subsection{Research and Analysis}

Research and analysis take time. Time to understand and implement solutions, and it seems the more a solution is developed, the wider the view of possible improvements is seen. The limitation of time taught that early experiments can enlighten the subject by creating quantifiably comparable viewpoints. Each approach has its pros and cons, and using time wisely helps distinguish broad and flexible long term solutions from short term hacks. 

From a high level, analysis, research and experience seem to be indistinguishable, feeding back into each other after iterative improvements. Practical coding leads to evaluation results and analysis, which cyclically creates additional hypotheses. Knowing when to stop, and receiving constructive criticism helps eliminate scenarios by accepting experience from colleagues. Being able to deeply comprehend and accept criticism and time constraints was important in creating a good solution.

Working on a research project provided initial methodologies for further works and instilled a deep interest in experimental tasks to perform. The task as a whole taught me that scientific methodologies can be applied in any hypothetical programmatic domain and inspired confidence in the ability to grasp experimental ideas. It gave me the tools to comprehend initial steps in analytical disciplines and the self-assurance to follow through.
